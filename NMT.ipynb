{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Reference: https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf <br>\n",
    "Original Notebook: https://github.com/nyu-dl/AMMI-2019-NLP-Part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Google Translate API for Comparison\n",
    "\n",
    "https://github.com/ssut/py-googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_utils = 'pyfiles'\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(path_to_utils)\n",
    "import nmt_dataset\n",
    "import nnet_models\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from functools import partial\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import copy\n",
    "from subword_nmt.apply_bpe import BPE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "We will work with a English to French Dataset from https://www.manythings.org/anki/.\n",
    "\n",
    "The data is downloaded by running the `download-data.sh` script. You can also modify this script to download data for other language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "source_lang, target_lang = 'en', 'fr'\n",
    "model_dir = 'models/{}-{}'.format(source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -5 data/train.en-fr.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data\n",
    "\n",
    "1. Load the BPE model\n",
    "2. Load the parallel corpora for this language pair (train, valid and test). `load_data` will load a corpus and tokenize it with the BPE model with the `preprocess` function.\n",
    "3. Create (or load) dictionaries that map BPE tokens to token IDs (`nmt_dataset.load_or_create_dictionary` function)\n",
    "4. Binarize the data: map source and target text sequences to sequences of IDs, and sort the training set by length (`nmt_dataset.binarize` function)\n",
    "5. Create batches (`nmt_dataset.BatchIterator` class): group multiple sequence pairs of similar length together, pad them to the maximum length and create numpy arrays that can be used to train our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=1234):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the BPE model (multilingual BPE model, works with French, German and English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = os.path.join(data_dir, 'bpecodes.de-en-fr')\n",
    "\n",
    "with open(bpe_path) as bpe_codes:\n",
    "    bpe_model = BPE(bpe_codes)\n",
    "\n",
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    return bpe_model.segment(line.lower())\n",
    "\n",
    "def postprocess(line):\n",
    "    return line.replace('@@ ', '')\n",
    "\n",
    "def load_data(source_lang, target_lang, split='train', max_size=None):\n",
    "    # max_size: max number of sentence pairs in the training corpus (None = all)\n",
    "    path = os.path.join(data_dir, '{}.{}-{}'.format(split, *sorted([source_lang, target_lang])))\n",
    "    return nmt_dataset.load_dataset(path, source_lang, target_lang, preprocess=preprocess, max_size=None)   # set max_size to 10000 for fast debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load and preprocess the parallel corpora (these are pandas DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(source_lang, target_lang, 'train', max_size=None)   # set max_size to 10000 for fast debugging\n",
    "valid_data = load_data(source_lang, target_lang, 'valid')\n",
    "test_data = load_data(source_lang, target_lang, 'test')\n",
    "print(train_data.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load or create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_dict_path = os.path.join(model_dir, 'dict.{}.txt'.format(source_lang))\n",
    "target_dict_path = os.path.join(model_dir, 'dict.{}.txt'.format(target_lang))\n",
    "\n",
    "source_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    source_dict_path,\n",
    "    train_data['source_tokenized'],\n",
    "    minimum_count=1,\n",
    "    reset=False    # set reset to True if you're changing the data or the preprocessing\n",
    ")\n",
    "print(source_dict.words[:100])\n",
    "\n",
    "target_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    target_dict_path,\n",
    "    train_data['target_tokenized'],\n",
    "    minimum_count=1,\n",
    "    reset=False\n",
    ")\n",
    "print(target_dict.words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('source vocab size:', len(source_dict))\n",
    "print('target vocab size:', len(target_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use the dictionaries to map tokens to indices. The training set is also sorted by length for more efficient batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmt_dataset.binarize(train_data, source_dict, target_dict, sort=True)\n",
    "nmt_dataset.binarize(valid_data, source_dict, target_dict, sort=False)\n",
    "nmt_dataset.binarize(test_data, source_dict, target_dict, sort=False)\n",
    "print(train_data.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_size={}, valid_size={}, test_size={}, min_len={}, max_len={}'.format(\n",
    "    len(train_data),\n",
    "    len(valid_data),\n",
    "    len(test_data),\n",
    "    train_data['source_len'].min(),\n",
    "    train_data['source_len'].max(),\n",
    "))\n",
    "\n",
    "print('Train source length distribution:')\n",
    "print(train_data['source_len'].quantile([0.5, 0.75, 0.9, 0.95, 0.99, 0.999, 0.9999]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build batches. The training batches are automatically shuffled before each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 30       # maximum 30 tokens per sentence (longer sequences will be truncated)\n",
    "batch_size = 512   # maximum 512 tokens per batch (decrease if you get OOM errors, increase to speed up training)\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "train_iterator = nmt_dataset.BatchIterator(train_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=True)\n",
    "valid_iterator = nmt_dataset.BatchIterator(valid_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)\n",
    "test_iterator = nmt_dataset.BatchIterator(test_data, source_lang, target_lang, batch_size=batch_size, max_len=max_len, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of training batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(train_iterator)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder-Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of usually of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence. Essentially, all we need is some mechanism to read the source sentence and create an encoding and some mechanism to read the encoding and decode it to the target language. \n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"I am not the\n",
    "black cat\" → \"Je ne suis pas le chat noir\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder is anything which takes in a sentence and gives us a representation for the sentence. \n",
    "\n",
    "The encoder of a seq2seq network can be a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "However, we will start with a simpler Bag-of-Words encoder and then move on to more complex encoders.\n",
    "\n",
    "### Bag-of-Words Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_encoder = nnet_models.BagOfWords(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    reduce=\"sum\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "--------------------\n",
    "\n",
    "The decoder is another network that takes the encoder output vector(s) and outputs a sequence of words to create the translation.\n",
    "\n",
    "### Decoder without Attention\n",
    "\n",
    "In the simplest seq2seq decoder we use only the last output of the encoder. This last output is sometimes called the context vector as it encodes context from the entire sequence. This context vector can be used as the initial hidden state for an RNN decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and hidden state. The initial input token is the start-of-string <SOS> token, and the first hidden state is the context vector (the encoder's last hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_decoder = nnet_models.RNN_Decoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bow_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_model = nnet_models.EncoderDecoder(\n",
    "    bow_encoder,\n",
    "    bow_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, checkpoint_path):\n",
    "    dirname = os.path.dirname(checkpoint_path)\n",
    "    if dirname:\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "    torch.save(model, checkpoint_path)\n",
    "\n",
    "def train_model(\n",
    "        train_iterator,\n",
    "        valid_iterators,\n",
    "        model,\n",
    "        checkpoint_path,\n",
    "        epochs=10,\n",
    "        validation_frequency=1\n",
    "    ):\n",
    "    \"\"\"\n",
    "    train_iterator: instance of nmt_dataset.BatchIterator or nmt_dataset.MultiBatchIterator\n",
    "    valid_iterators: list of nmt_dataset.BatchIterator\n",
    "    model: instance of nnet_models.EncoderDecoder\n",
    "    checkpoint_path: path of the model checkpoint\n",
    "    epochs: iterate this many times over train_iterator\n",
    "    validation_frequency: validate the model every N epochs\n",
    "    \"\"\"\n",
    "\n",
    "    reset_seed()\n",
    "\n",
    "    best_bleu = -1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        start = time.time()\n",
    "        running_loss = 0\n",
    "\n",
    "        print('Epoch: [{}/{}]'.format(epoch, epochs))\n",
    "\n",
    "        # Iterate over training batches for one epoch\n",
    "        for i, batch in tqdm(enumerate(train_iterator), total=len(train_iterator)):\n",
    "            t = time.time()\n",
    "            running_loss += model.train_step(batch)\n",
    "\n",
    "        # Average training loss for this epoch\n",
    "        epoch_loss = running_loss / len(train_iterator)\n",
    "\n",
    "        print(\"loss={:.3f}, time={:.2f}\".format(epoch_loss, time.time() - start))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Evaluate and save the model\n",
    "        if epoch % validation_frequency == 0:\n",
    "            bleu_scores = []\n",
    "            \n",
    "            # Compute BLEU over all validation sets\n",
    "            for valid_iterator in valid_iterators:\n",
    "                src, tgt = valid_iterator.source_lang, valid_iterator.target_lang\n",
    "                translation_output = model.translate(valid_iterator, postprocess)\n",
    "                bleu_score = translation_output.score\n",
    "                output = translation_output.output\n",
    "\n",
    "                with open(os.path.join(model_dir, 'valid.{}-{}.{}.out'.format(src, tgt, epoch)), 'w') as f:\n",
    "                    f.writelines(line + '\\n' for line in output)\n",
    "\n",
    "                print('{}-{}: BLEU={}'.format(src, tgt, bleu_score))\n",
    "                sys.stdout.flush()\n",
    "                bleu_scores.append(bleu_score)\n",
    "\n",
    "            # Average the validation BLEU scores\n",
    "            bleu_score = round(sum(bleu_scores) / len(bleu_scores), 2)\n",
    "            if len(bleu_scores) > 1:\n",
    "                print('BLEU={}'.format(bleu_score))\n",
    "\n",
    "            # Update the model's learning rate based on current performance.\n",
    "            # This scheduler divides the learning rate by 10 if BLEU does not improve.\n",
    "            model.scheduler_step(bleu_score)\n",
    "\n",
    "            # Save a model checkpoint if it has the best validation BLEU so far\n",
    "            if bleu_score > best_bleu:\n",
    "                best_bleu = bleu_score\n",
    "                save_model(model, checkpoint_path)\n",
    "\n",
    "        print('=' * 50)\n",
    "\n",
    "    print(\"Training completed. Best BLEU is {}\".format(best_bleu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with BOW Encoder and RNN Decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epoch\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'bow.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(model_dir, 'pretrained-bow.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    bow_model = torch.load(checkpoint_path)\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], bow_model,\n",
    "                epochs=10,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU:', bow_model.translate(test_iterator, postprocess).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binned_bleu_scores(model, valid_iterator):\n",
    "    # Compute and plot BLEU scores according to sequence length\n",
    "    # lengths = np.arange(0, 31, 5)\n",
    "    lengths = np.arange(4, 20, 3)\n",
    "    bleu_scores = np.zeros(len(lengths))\n",
    "\n",
    "    for i in tqdm(range(1, len(lengths)), total=len(lengths) - 1):\n",
    "        min_len = lengths[i - 1]\n",
    "        max_len = lengths[i]\n",
    "\n",
    "        tmp_data = valid_data[(valid_iterator.data['source_len'] > min_len) & (valid_iterator.data['source_len'] <= max_len)]\n",
    "        tmp_iterator = nmt_dataset.BatchIterator(tmp_data, source_lang, target_lang, batch_size, max_len=max_len)\n",
    "\n",
    "        bleu_scores[i] = model.translate(tmp_iterator, postprocess).score\n",
    "\n",
    "    lengths = lengths[1:]\n",
    "    bleu_scores = bleu_scores[1:]\n",
    "\n",
    "    plt.plot(lengths, bleu_scores, 'x-')\n",
    "    plt.ylim(0, np.max(bleu_scores) + 1)\n",
    "    plt.xlabel('Source length')\n",
    "    plt.ylabel('BLEU score')\n",
    "    \n",
    "    return lengths, bleu_scores\n",
    "\n",
    "\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Plot an encoder-decoder attention matrix\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone', aspect='auto')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       [nmt_dataset.EOS_TOKEN], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' ') +\n",
    "                       [nmt_dataset.EOS_TOKEN])\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def encode_as_batch(sentence, dictionary, source_lang, target_lang):\n",
    "    # Create a batch from a single sentence\n",
    "    sentence = sentence + ' ' + nmt_dataset.EOS_TOKEN\n",
    "    tensor = dictionary.txt2vec(sentence).unsqueeze(0)\n",
    "\n",
    "    return {\n",
    "        'source': tensor,\n",
    "        'source_len': torch.from_numpy(np.array([tensor.shape[-1]])),\n",
    "        'source_lang': source_lang,\n",
    "        'target_lang': target_lang\n",
    "    }\n",
    "\n",
    "\n",
    "def get_translation(model, sentence, dictionary, source_lang, target_lang, return_output=False):\n",
    "    # Translate given sentence with given model. Also show translation outputs by Google Translate for comparison.\n",
    "    print('Source:', sentence)\n",
    "    sentence_tok = preprocess(sentence, is_source=True, source_lang=source_lang, target_lang=target_lang)\n",
    "    print('Tokenized source:', sentence_tok)\n",
    "    batch = encode_as_batch(sentence_tok, dictionary, source_lang, target_lang)\n",
    "    prediction, attn_matrix, enc_self_attn = model.eval_step(batch)\n",
    "    prediction = prediction[0]\n",
    "    prediction_detok = postprocess(prediction)\n",
    "    print('Prediction:', prediction)\n",
    "    print('Detokenized prediction:', prediction_detok)\n",
    "\n",
    "    print('Google Translate ({}->{}): {}'.format(\n",
    "        source_lang,\n",
    "        target_lang,\n",
    "        translator.translate(sentence, src=source_lang, dest=target_lang).text\n",
    "    ))\n",
    "    print('Google Translate on prediction ({}->{}): {}'.format(\n",
    "        target_lang,\n",
    "        source_lang,\n",
    "        translator.translate(prediction_detok, src=target_lang, dest=source_lang).text\n",
    "    ))\n",
    "\n",
    "    results = {\n",
    "        'source': sentence,\n",
    "        'source_tokens': sentence_tok.split(' ') + ['<eos>'],\n",
    "        'prediction_detok': prediction_detok,\n",
    "        'prediction_tokens': prediction.split(' '),\n",
    "    }\n",
    "\n",
    "    if attn_matrix is not None:\n",
    "        attn_matrix = attn_matrix[0].detach().cpu().numpy()\n",
    "        results['attention_matrix'] = attn_matrix\n",
    "        show_attention(sentence_tok, prediction, attn_matrix)\n",
    "    \n",
    "    if enc_self_attn is not None:\n",
    "        results['encoder_self_attention_list'] = enc_self_attn\n",
    "    \n",
    "    if return_output:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(bow_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest limitation of a Bag-of-Word encoder is that is insensitive to word order: <br>\n",
    "when shuffling the words in the previous sentence, you get the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(bow_model, 'are hello ? how you', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(bow_model, 'she \\'s five years older than me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder + RNN Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_encoder = nnet_models.RNN_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_decoder = nnet_models.RNN_Decoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = nnet_models.EncoderDecoder(\n",
    "    rnn_encoder,\n",
    "    rnn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN Encoder and RNN Decoder (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epoch\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(model_dir, 'pretrained-rnn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_model = torch.load(checkpoint_path)\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], rnn_model,\n",
    "                epochs=10,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU:', rnn_model.translate(test_iterator, postprocess).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to the BoW encoder, an RNN is sensitive to word ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'are hello ? how you', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'she \\'s five years older than me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot validation BLEU according to source sequence length\n",
    "The performance quickly degrades as the input length increases. This is caused by three main factors:\n",
    "- The RNN decoder (without attention) only relies on the last hidden state of the encoder. This means that we have to encode the full sentence into a single fixed-size vector\n",
    "- Encoder-decoder RNNs are difficult to train (because the signal has to be backpropagated through the entire sequence of states)\n",
    "- The training set we used is mostly composed of very short sentences (95% of source sentences are 15 tokens or less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_lengths, rnn_bleu_scores = get_binned_bleu_scores(rnn_model, valid_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Encoder + RNN Decoder with Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_encoder = nnet_models.RNN_Encoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(rnn_attn_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_decoder = nnet_models.AttentionDecoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    dropout=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnn_attn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_model = nnet_models.EncoderDecoder(\n",
    "    rnn_attn_encoder,\n",
    "    rnn_attn_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model with RNN Encoder and RNN Decoder with attention (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epoch\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'rnn-attn.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(model_dir, 'pretrained-rnn-attn.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    rnn_attn_model = torch.load(checkpoint_path)\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], rnn_attn_model,\n",
    "                epochs=10,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU:', rnn_attn_model.translate(test_iterator, postprocess).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot validation BLEU according to source sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_attn_lengths, rnn_attn_bleu_scores = get_binned_bleu_scores(rnn_attn_model, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rnn_lengths, rnn_bleu_scores, '--x', label='RNN without attention')\n",
    "plt.plot(rnn_attn_lengths, rnn_attn_bleu_scores, '--x', label='RNN with attention')\n",
    "plt.xlabel('Source length')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model and visualize attention matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'hello how are you ?', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'she \\'s five years older than me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(rnn_attn_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "[Transformer](https://arxiv.org/abs/1706.03762) is currently the state-of-the-art for Machine Translation. The encoder uses self-attention over the previous layers. The decoder combines self-attention and encoder-decoder attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_encoder = nnet_models.TransformerEncoder(\n",
    "    input_size=len(source_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    heads=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_decoder = nnet_models.TransformerDecoder(\n",
    "    output_size=len(target_dict),\n",
    "    hidden_size=512,\n",
    "    num_layers=1,\n",
    "    heads=4,\n",
    "    dropout=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformer_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = nnet_models.EncoderDecoder(\n",
    "    transformer_encoder,\n",
    "    transformer_decoder,\n",
    "    lr=0.001,\n",
    "    use_cuda=True,\n",
    "    target_dict=target_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Transformer model (or load a pre-trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this value to True to train your own model. By default, a pre-trained model will be loaded.\n",
    "# Tip: you can set \"epoch\" to a small value (e.g., 2) and re-run this cell several times to continue training you model (`train_model` does not reset the model)\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "    checkpoint_path = os.path.join(model_dir, 'transformer.pt')\n",
    "else:\n",
    "    checkpoint_path = os.path.join(model_dir, 'pretrained-transformer.pt')\n",
    "\n",
    "print('checkpoint path:', checkpoint_path)\n",
    "\n",
    "if os.path.exists(checkpoint_path) and not train_again:\n",
    "    transformer_model = torch.load(checkpoint_path)\n",
    "else:\n",
    "    train_model(train_iterator, [valid_iterator], transformer_model,\n",
    "                epochs=10,\n",
    "                checkpoint_path=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute BLEU on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU:', transformer_model.translate(valid_iterator, postprocess).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot validation BLEU according to source sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_lengths, transformer_bleu_scores = get_binned_bleu_scores(transformer_model, valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rnn_lengths, rnn_bleu_scores, '--x', label='RNN without attention')\n",
    "plt.plot(transformer_lengths, transformer_bleu_scores, '--x', label='Transformer')\n",
    "plt.xlabel('Source length')\n",
    "plt.ylabel('BLEU score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz.bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n",
    "    jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_head_view(results):\n",
    "    self_attention = results['encoder_self_attention_list']\n",
    "    tokens = results['source_tokens']\n",
    "    sentence_b_start = None\n",
    "    head_view(self_attention, tokens, sentence_b_start)\n",
    "\n",
    "def show_model_view(results):\n",
    "    self_attention = results['encoder_self_attention_list']\n",
    "    tokens = results['source_tokens']\n",
    "    sentence_b_start = None\n",
    "    model_view(self_attention, tokens, sentence_b_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_translation(transformer_model, 'hello how are you ?', source_dict, source_lang, target_lang, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_head_view(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model_view(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_translation(transformer_model, 'she \\'s five years older than me .', source_dict, source_lang, target_lang, return_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_translation(transformer_model, 'i know that the last thing you want to do is help me .', source_dict, source_lang, target_lang, return_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilingual Transformer model\n",
    "\n",
    "Load a pre-trained **de, fr <-> en** model. The same dictionary and embeddings are shared between all languages, and language codes (`<lang:de>`, `<lang:en>`, `<lang:fr>`) are prepended to each source sequence to identify the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model_dir = os.path.join('models', 'de-en-fr')\n",
    "\n",
    "multi_dict = nmt_dataset.load_or_create_dictionary(\n",
    "    os.path.join(multi_model_dir, 'dict.txt'),\n",
    "    dataset=None,\n",
    "    minimum_count=10,\n",
    "    reset=False\n",
    ")\n",
    "\n",
    "checkpoint_path = os.path.join(multi_model_dir, 'pretrained-transformer.pt')\n",
    "multi_transformer_model = torch.load(checkpoint_path)\n",
    "\n",
    "print(multi_transformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual evaluation\n",
    "\n",
    "Modify the `preprocess` function to automatically prepend language codes to all source sequences (when calling `get_translation`, or `load_data`).\n",
    "\n",
    "And load test sets in all language pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line, is_source=True, source_lang=None, target_lang=None):\n",
    "    line = bpe_model.segment(line.lower())\n",
    "    if is_source:\n",
    "        line = '<lang:{}> {}'.format(target_lang, line)\n",
    "    return line\n",
    "\n",
    "test_iterators = []\n",
    "\n",
    "for src, tgt in ('en', 'fr'), ('fr', 'en'), ('en', 'de'), ('de', 'en'), ('de', 'fr'), ('fr', 'de'):\n",
    "    dataset = load_data(src, tgt, 'test')\n",
    "    nmt_dataset.binarize(dataset, source_dict=multi_dict, target_dict=multi_dict, sort=False)\n",
    "    iterator = nmt_dataset.BatchIterator(dataset, src, tgt, batch_size=512, max_len=30, shuffle=False)\n",
    "    test_iterators.append(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_iterator in test_iterators[:4]:\n",
    "    print('BLEU {}-{}: {}'.format(\n",
    "        test_iterator.source_lang,\n",
    "        test_iterator.target_lang,\n",
    "        multi_transformer_model.translate(test_iterator, postprocess).score\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_transformer_model, 'she \\'s five years older than me .', multi_dict, source_lang='en', target_lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_transformer_model, 'sie ist fünf jahre älter als ich .', multi_dict, source_lang='de', target_lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot translation\n",
    "\n",
    "In theory, the model can do **zero-shot** translation, i.e., translate between German and French even though it has never seen German-French sentence pairs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_iterator in test_iterators[4:]:\n",
    "    print('BLEU {}-{}: {}'.format(\n",
    "        test_iterator.source_lang,\n",
    "        test_iterator.target_lang,\n",
    "        multi_transformer_model.translate(test_iterator, postprocess).score\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, in practice zero-shot performance is very bad. Interact with the model to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_transformer_model, 'sie ist fünf jahre älter als ich .', multi_dict, 'de', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_translation(multi_transformer_model, 'elle a cinq ans de plus que moi .', multi_dict, 'fr', 'de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "\n",
    "Choose one of these exercises, or both!\n",
    "\n",
    "### Hyper-parameter tuning\n",
    "\n",
    "Find the best hyper-parameters for Transformer **en-fr**. Share your best test BLEU scores on Slack!\n",
    "\n",
    "*Don't forget to reload the `preprocess` function at the start of the notebook*\n",
    "\n",
    "- Hyper-parameters: `lr`, `batch_size`, `num_layers`, `hidden_size`, `dropout`, `heads`, etc.\n",
    "- Other improvements: modify the learning rate scheduler and optimizer in `nnet_models.EncoderDecoder`; use different embedding size and hidden size, etc.\n",
    "\n",
    "### Multilingual NMT\n",
    "\n",
    "Train your own multilingual NMT model.\n",
    "\n",
    "Tips:\n",
    "- Create a multilingual dictionary by concatenating the tokenized data in all languages. Or simply re-use the dictionary of the pre-trained model (`multi_dict`).\n",
    "- Use the same dictionary for the source and target sides, and share the embeddings between your encoder and decoder (do: `decoder.embedding = encoder.embedding`).\n",
    "- Use `nmt_dataset.MultiBatchIterator(iterator_list)` to concatenate a list of training iterators (one for each language pair) into a single iterator, which is compatible with `train_model`.\n",
    "- `train_model` can take a list of several validation iterators, which will let you validate your model on several language pairs.\n",
    "- Improve your model's performance on **de-fr** and **fr-de** by including training data for these languages pairs (`data/train.de-fr.de` and `data/train.de-fr.fr`).\n",
    "\n",
    "### Train bilingual or multilingual models on other language pairs\n",
    "\n",
    "- Modify and re-run `./download-data.sh` to download data in new languages, preprocess this data and train BPE models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}